## Spark数据倾斜

#### 什么是数据倾斜?

就是当我们的数据按照key进行分区时,数据中相同的key特别多,只要key相同,就一定会被分到同一个分区中,这就导致了某个分区要处理的数巨量特别大,而有的分区处理的

数据量特别小,就会导致数据量大的分区处理时间加长,拖延整个程序的运行时间,而有的分区又被闲置,不能有效地利用资源.

解决方法 : 我们要在数据进行分区之前对数据的key重新设计,然后分区之后,到达聚合阶段在把key还原成我们想要的样子.

具体方法: 比如有100个分区,我们可以设置一个1~100的随机数,然后把数据的key加上下划线后跟着我们的随机数,这样所有的key都会被均匀的分到所有的分区中,
等到业务完成,到聚合阶段先对key还原.
